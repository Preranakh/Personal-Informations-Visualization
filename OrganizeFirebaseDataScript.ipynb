{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preranakh/Personal-Informations-Visualization/blob/main/OrganizeFirebaseDataScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tViWOduCt92Y",
        "outputId": "28fea896-4298-42d3-ae69-c7446f814f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw_MyrpZdobv",
        "outputId": "38249b59-d761-4b48-bc25-c7e29350cb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m122.9/154.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def convert_time_to_seconds(time_str):\n",
        "    mins, secs = 0, 0\n",
        "    if \"Mins\" in time_str:\n",
        "        mins = int(time_str.split(\"Mins\")[0].strip())\n",
        "        secs = int(time_str.split(\"Mins\")[1].split(\"Secs\")[0].strip().split(\":\")[1])\n",
        "    else:\n",
        "        secs = int(time_str.split(\"Secs\")[0].strip())\n",
        "    return mins * 60 + secs\n",
        "\n",
        "with open('database1.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "user_data = data['user_session_data_test']\n",
        "\n",
        "control_group_ids = [\n",
        "    \"izZlxECbEvbZbaQsiTC2IppTR1D2\", \"w1bQfQVpS0e2bdMS0mgXpSHqZew2\", \"Uah5QqbULfOwCEwuKqEmRv7ELx42\",\n",
        "    \"l1JBi3rjs7eEzKECNW02rhKkzI12\", \"kt6wMrsVN1YNwymFsZCPha4eG6w1\", \"gSznvtvnoeMV3bvm9aMP0uwlUhl2\",\n",
        "    \"ZbmqqGFDwmZXkjR9lxJRB8Y6tmd2\", \"U2HjmAv7KCcfQVHsr7ofVPdaTm82\", \"n3mtRWzHDRSm9NYqoGxyyM7zyuQ2\",\n",
        "    \"Vv3J8PwWWeSWErREMVtrxwkP8rF3\", \"ogFBk6o71cWw7TQejwgzYWYdetK2\", \"YOlWWzDFhyUJuCCazPl7vMdPLj23\",\n",
        "    \"wrGFkJdPKwNNcOIDSPmN62VGIzC2\", \"g6RotBsUG0fQVfkSampcMTaWdTE3\", \"2blzSKV1tkcSFq2f8opLrcM5iP33\",\n",
        "    \"EfVmI4iVenXsMAX392YBkIgysH23\", \"lJNN7pxBDhTulyLFLr70FghZLfN2\", \"6X2UCYgJSBV0dFWD0yWYodEmNKu2\"\n",
        "]\n",
        "\n",
        "intervention_group_ids = [\n",
        "    \"3YPGBN3vD3TIqg3Jlrxgu16gj4K3\", \"ehQq99p6woP538JE31l8s0lcBr62\", \"NMUVcKxsfbT76avSebXt6fmsKw02\",\n",
        "    \"iVMz6gl0vkWNZuTJ64bUXEj1AJm1\", \"K9IJNZxN3OUNiOn6kFuia77Io9m2\", \"ttrSxxnr5ROCWCynB2kC2cJgkEr1\",\n",
        "    \"ZXkLCiyTgyXJGmL5dXF7Pip5Tpk2\", \"qf7BdXXKWTNisXiY3dNGynktezC3\", \"6AfBZNSFoDYtFZoK3v6IoDzYGX23\",\n",
        "    \"DMRsCjYfw3Ui2WY7baPfiliLmYm1\", \"yOwa5JFnOFcPhY68B9JfQ8FXhqN2\", \"RXrptwB3hBeMx77a0g09or8T7zk2\",\n",
        "    \"4N71ZXMaaTcJbEmeUAg1KrZlQNA3\", \"iXcMJZtl9wXfvb5elAKio2Nrn0D2\", \"OIf29ayJNJUCcBRExfWG27W9iCh1\",\n",
        "    \"okJYh5CBOrbVuzpP7dLOiBWxGuj1\", \"i3HmbHac0eQp6D3iC2QMRtNIs7a2\", \"Et7CFW8jkcX1GRjETBPcGsy42kW2\",\n",
        "    \"zFJFxFXuJhTjC7oRQ30u8UJIFLF2\"\n",
        "]\n",
        "\n",
        "groups = {\n",
        "    'Control Group': control_group_ids,\n",
        "    'Intervention Group': intervention_group_ids\n",
        "}\n",
        "\n",
        "true_articles = ['A2', 'A4', 'A6']\n",
        "fake_articles = ['A1', 'A3', 'A5']\n",
        "\n",
        "with pd.ExcelWriter('output.xlsx') as writer:\n",
        "    for group_name, group_ids in groups.items():\n",
        "        all_rows = []\n",
        "        for user_id, user_values in user_data.items():\n",
        "            if user_id in group_ids:\n",
        "                rows = []\n",
        "                first_entry_for_user = True  # A flag to keep track of first entry for a user\n",
        "                for date, articles in user_values.items():\n",
        "                    for url_encoded, metrics in articles['articles'].items():\n",
        "                        url = url_encoded.replace('%', '/').replace(' ', '.')\n",
        "                        row = [\n",
        "                            url,\n",
        "                            metrics['article_id'],\n",
        "                            metrics['article_read'],\n",
        "                            metrics['avg_reading_time'],\n",
        "                            metrics['click_rate'],\n",
        "                            metrics['current_article_bias'],\n",
        "                            metrics['current_reading_time'],\n",
        "                            convert_time_to_seconds(metrics['current_reading_time']),\n",
        "                            metrics['dominant_bias'],\n",
        "                            metrics['page_title'],\n",
        "                            metrics['scroll_rate'],\n",
        "                            metrics['source'],\n",
        "                            user_id if first_entry_for_user else ''  # Only add UserID for the first row of this user\n",
        "                        ]\n",
        "                        first_entry_for_user = False  # Set the flag to false after the first entry\n",
        "                        rows.append(row)\n",
        "                all_rows.extend(rows + [[''] * 13])  # Adding an empty row between different UserIDs\n",
        "\n",
        "        df_group = pd.DataFrame(all_rows, columns=[\n",
        "            \"URL\", \"article_id\", \"article_read\", \"avg_reading_time\", \"click_rate\",\n",
        "            \"current_article_bias\", \"current_reading_time\", \"current_reading_time (seconds)\",\n",
        "            \"dominant_bias\", \"page_title\", \"scroll_rate\", \"source\", \"UserID\"\n",
        "        ])\n",
        "\n",
        "        # Add a space before the \"UserID\" column\n",
        "        df_group['source'] = df_group['source'].astype(str) + ' '\n",
        "\n",
        "        df_group['current_reading_time (seconds)'] = pd.to_numeric(df_group['current_reading_time (seconds)'], errors='coerce')\n",
        "\n",
        "        # Calculate the desired metrics\n",
        "        avg_reading_time = df_group['current_reading_time (seconds)'].mean()\n",
        "        avg_true_article_time = df_group[df_group['article_id'].isin(true_articles)]['current_reading_time (seconds)'].mean()\n",
        "        avg_fake_article_time = df_group[df_group['article_id'].isin(fake_articles)]['current_reading_time (seconds)'].mean()\n",
        "\n",
        "        # Print the metrics to the Excel sheet\n",
        "        df_metrics = pd.DataFrame({\n",
        "            'Metric': ['Average Current Reading Time', 'Average Current Reading Time for True Articles', 'Average Current Reading Time for Fake Articles'],\n",
        "            'Value': [avg_reading_time, avg_true_article_time, avg_fake_article_time]\n",
        "        })\n",
        "\n",
        "        # Write to Excel file\n",
        "        df_group.to_excel(writer, sheet_name=f'{group_name} Data', index=False)\n",
        "        df_metrics.to_excel(writer, sheet_name=f'{group_name} Metrics', index=False)\n",
        "\n",
        "print(\"Data written to output.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB70eskRemRA",
        "outputId": "e1721cff-45d4-4996-ccc0-3d5679abde16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data written to output.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def convert_time_to_seconds(time_str):\n",
        "    mins, secs = 0, 0\n",
        "    if \"Mins\" in time_str:\n",
        "        mins = int(time_str.split(\"Mins\")[0].strip())\n",
        "        secs = int(time_str.split(\"Mins\")[1].split(\"Secs\")[0].strip().split(\":\")[1])\n",
        "    else:\n",
        "        secs = int(time_str.split(\"Secs\")[0].strip())\n",
        "    return mins * 60 + secs\n",
        "\n",
        "with open('database1.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "user_data = data['user_session_data_test']\n",
        "user_log_data = data.get('user_log', {})\n",
        "\n",
        "control_group_ids = [\n",
        "    \"izZlxECbEvbZbaQsiTC2IppTR1D2\", \"w1bQfQVpS0e2bdMS0mgXpSHqZew2\", \"Uah5QqbULfOwCEwuKqEmRv7ELx42\",\n",
        "    \"l1JBi3rjs7eEzKECNW02rhKkzI12\", \"kt6wMrsVN1YNwymFsZCPha4eG6w1\", \"gSznvtvnoeMV3bvm9aMP0uwlUhl2\",\n",
        "    \"ZbmqqGFDwmZXkjR9lxJRB8Y6tmd2\", \"U2HjmAv7KCcfQVHsr7ofVPdaTm82\", \"n3mtRWzHDRSm9NYqoGxyyM7zyuQ2\",\n",
        "    \"Vv3J8PwWWeSWErREMVtrxwkP8rF3\", \"ogFBk6o71cWw7TQejwgzYWYdetK2\", \"YOlWWzDFhyUJuCCazPl7vMdPLj23\",\n",
        "    \"wrGFkJdPKwNNcOIDSPmN62VGIzC2\", \"g6RotBsUG0fQVfkSampcMTaWdTE3\", \"2blzSKV1tkcSFq2f8opLrcM5iP33\",\n",
        "    \"EfVmI4iVenXsMAX392YBkIgysH23\", \"lJNN7pxBDhTulyLFLr70FghZLfN2\", \"6X2UCYgJSBV0dFWD0yWYodEmNKu2\"\n",
        "]\n",
        "\n",
        "intervention_group_ids = [\n",
        "    \"3YPGBN3vD3TIqg3Jlrxgu16gj4K3\", \"ehQq99p6woP538JE31l8s0lcBr62\", \"NMUVcKxsfbT76avSebXt6fmsKw02\",\n",
        "    \"iVMz6gl0vkWNZuTJ64bUXEj1AJm1\", \"K9IJNZxN3OUNiOn6kFuia77Io9m2\", \"ttrSxxnr5ROCWCynB2kC2cJgkEr1\",\n",
        "    \"ZXkLCiyTgyXJGmL5dXF7Pip5Tpk2\", \"qf7BdXXKWTNisXiY3dNGynktezC3\", \"6AfBZNSFoDYtFZoK3v6IoDzYGX23\",\n",
        "    \"DMRsCjYfw3Ui2WY7baPfiliLmYm1\", \"yOwa5JFnOFcPhY68B9JfQ8FXhqN2\", \"RXrptwB3hBeMx77a0g09or8T7zk2\",\n",
        "    \"4N71ZXMaaTcJbEmeUAg1KrZlQNA3\", \"iXcMJZtl9wXfvb5elAKio2Nrn0D2\", \"OIf29ayJNJUCcBRExfWG27W9iCh1\",\n",
        "    \"okJYh5CBOrbVuzpP7dLOiBWxGuj1\", \"i3HmbHac0eQp6D3iC2QMRtNIs7a2\", \"Et7CFW8jkcX1GRjETBPcGsy42kW2\",\n",
        "    \"zFJFxFXuJhTjC7oRQ30u8UJIFLF2\"\n",
        "]\n",
        "\n",
        "groups = {\n",
        "    'Control Group': control_group_ids,\n",
        "    'Intervention Group': intervention_group_ids\n",
        "}\n",
        "\n",
        "\n",
        "with pd.ExcelWriter('output.xlsx') as writer:\n",
        "    for group_name, group_ids in groups.items():\n",
        "        all_rows = []\n",
        "        for user_id, user_values in user_data.items():\n",
        "            if user_id in group_ids:\n",
        "                rows = []\n",
        "                known_urls = set()\n",
        "                first_entry_for_user = True\n",
        "\n",
        "                additional_urls = set()  # Store additional URLs for this user\n",
        "\n",
        "                for date, articles in user_values.items():\n",
        "                    for url_encoded, metrics in articles['articles'].items():\n",
        "                        url = url_encoded.replace('%', '/').replace(' ', '.')\n",
        "                        known_urls.add(url)\n",
        "                        row = [\n",
        "                            url,\n",
        "                            metrics['article_id'],\n",
        "                            metrics['article_read'],\n",
        "                            metrics['avg_reading_time'],\n",
        "                            metrics['click_rate'],\n",
        "                            metrics['current_article_bias'],\n",
        "                            metrics['current_reading_time'],\n",
        "                            convert_time_to_seconds(metrics['current_reading_time']),\n",
        "                            metrics['dominant_bias'],\n",
        "                            metrics['page_title'],\n",
        "                            metrics['scroll_rate'],\n",
        "                            metrics['source'],\n",
        "                            user_id if first_entry_for_user else ''  # Only add UserID for the first row of this user\n",
        "                        ]\n",
        "                        first_entry_for_user = False\n",
        "                        rows.append(row)\n",
        "\n",
        "                # Check for additional URLs from user_log\n",
        "                user_log = user_log_data.get(user_id, {})\n",
        "\n",
        "                for date, log in user_log.items():\n",
        "                    for url_encoded in log.get('articles', []):\n",
        "                        url = url_encoded.replace('%', '/').replace(' ', '.')\n",
        "                        if url not in known_urls:\n",
        "                            additional_urls.add(url)\n",
        "\n",
        "                # Add the additional URLs as a column to the first row (if they exist)\n",
        "                if rows and additional_urls:\n",
        "                    rows[0].append(' | '.join(additional_urls))\n",
        "                elif additional_urls:\n",
        "                    rows.append([''] * 12 + [user_id] + [' | '.join(additional_urls)])\n",
        "                else:\n",
        "                    # If no additional URLs, just append an empty space\n",
        "                    if rows:\n",
        "                        rows[0].append('')\n",
        "\n",
        "                all_rows.extend(rows + [[''] * 14])\n",
        "\n",
        "        df_group = pd.DataFrame(all_rows, columns=[\n",
        "            \"URL\", \"article_id\", \"article_read\", \"avg_reading_time\", \"click_rate\",\n",
        "            \"current_article_bias\", \"current_reading_time\", \"current_reading_time (seconds)\",\n",
        "            \"dominant_bias\", \"page_title\", \"scroll_rate\", \"source\", \"UserID\", \"Additional URLs\"\n",
        "        ])\n",
        "\n",
        "        df_group['source'] = df_group['source'].astype(str) + ' '\n",
        "        df_group['current_reading_time (seconds)'] = pd.to_numeric(df_group['current_reading_time (seconds)'], errors='coerce')\n",
        "\n",
        "        # Calculate the desired metrics\n",
        "        avg_reading_time = df_group['current_reading_time (seconds)'].mean()\n",
        "        avg_true_article_time = df_group[df_group['article_id'].isin(true_articles)]['current_reading_time (seconds)'].mean()\n",
        "        avg_fake_article_time = df_group[df_group['article_id'].isin(fake_articles)]['current_reading_time (seconds)'].mean()\n",
        "\n",
        "        df_metrics = pd.DataFrame({\n",
        "            'Metric': ['Average Current Reading Time', 'Average Current Reading Time for True Articles', 'Average Current Reading Time for Fake Articles'],\n",
        "            'Value': [avg_reading_time, avg_true_article_time, avg_fake_article_time]\n",
        "        })\n",
        "\n",
        "        df_group.to_excel(writer, sheet_name=f'{group_name} Data', index=False)\n",
        "        df_metrics.to_excel(writer, sheet_name=f'{group_name} Metrics', index=False)\n",
        "\n",
        "print(\"Data written to output.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC4F5cFVyFwG",
        "outputId": "0e256a1a-164d-4696-cbeb-777504479d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data written to output.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def convert_time_to_seconds(time_str):\n",
        "    mins, secs = 0, 0\n",
        "    if \"Mins\" in time_str:\n",
        "        mins = int(time_str.split(\"Mins\")[0].strip())\n",
        "        secs = int(time_str.split(\"Mins\")[1].split(\"Secs\")[0].strip().split(\":\")[1])\n",
        "    else:\n",
        "        secs = int(time_str.split(\"Secs\")[0].strip())\n",
        "    return mins * 60 + secs\n",
        "\n",
        "with open('database.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "user_data = data['user_session_data_test']\n",
        "user_log_data = data.get('user_log', {})\n",
        "\n",
        "control_group_ids = [\n",
        "    \"m7y3ik7wygMz18y53q24cRQ0IUm1\", \"MNLmUNyuoAhsctnRCDyDFUqdk5O2\", \"izZlxECbEvbZbaQsiTC2IppTR1D2\",\n",
        "    \"w1bQfQVpS0e2bdMS0mgXpSHqZew2\", \"Uah5QqbULfOwCEwuKqEmRv7ELx42\", \"l1JBi3rjs7eEzKECNW02rhKkzI12\",\n",
        "    \"kt6wMrsVN1YNwymFsZCPha4eG6w1\", \"gSznvtvnoeMV3bvm9aMP0uwlUhl2\", \"ZbmqqGFDwmZXkjR9lxJRB8Y6tmd2\",\n",
        "    \"U2HjmAv7KCcfQVHsr7ofVPdaTm82\", \"n3mtRWzHDRSm9NYqoGxyyM7zyuQ2\", \"Vv3J8PwWWeSWErREMVtrxwkP8rF3\",\n",
        "    \"ogFBk6o71cWw7TQejwgzYWYdetK2\", \"YOlWWzDFhyUJuCCazPl7vMdPLj23\", \"wrGFkJdPKwNNcOIDSPmN62VGIzC2\",\n",
        "    \"g6RotBsUG0fQVfkSampcMTaWdTE3\", \"2blzSKV1tkcSFq2f8opLrcM5iP33\", \"EfVmI4iVenXsMAX392YBkIgysH23\",\n",
        "    \"lJNN7pxBDhTulyLFLr70FghZLfN2\", \"5iUhzT0PbYXwnwqxtV5noQBi9F63\", \"yRsUxHOBhrfNgCBI5qGRStwjmro1\",\n",
        "    \"jVo6tCQtDBX87xspQtRi6CYKFlA3\", \"niurytWFevdaI7ChIhrnEikzcqG3\", \"kEkfvtxjKdadzzgY2pUbDdBCTBt1\",\n",
        "    \"5uu5xIx85QOQQHNwg0gzw3UUiBC2\", \"HfC9NyJvCEQHuuE9xRkVPZrlmLx2\", \"ImsJdRqkckRP7TbNrWMB4JMRSJZ2\",\n",
        "    \"kYcjrZMDvXW3ZOdxVNN24WcaUQ43\", \"p3MbZA6YoIUEjhsPskBJunxy9Ro1\", \"xiPdEybZlCY9A5BXwjfTpdkSIZn1\",\n",
        "    \"WFU91jLayNV8GMzfEpqqy08P7d13\", \"yhoSSD2xIeWH8gnQ616SwtqEFKm2\", \"TCxKEnBv8zdsCF92blKac9rz8JB3\",\n",
        "]\n",
        "\n",
        "intervention_group_ids = [\n",
        "    \"O5zQix01toRNApLU1xzl8m2IUoa2\", \"3YPGBN3vD3TIqg3Jlrxgu16gj4K3\", \"ehQq99p6woP538JE31l8s0lcBr62\",\n",
        "    \"NMUVcKxsfbT76avSebXt6fmsKw02\", \"iVMz6gl0vkWNZuTJ64bUXEj1AJm1\", \"K9IJNZxN3OUNiOn6kFuia77Io9m2\",\n",
        "    \"ttrSxxnr5ROCWCynB2kC2cJgkEr1\", \"ZXkLCiyTgyXJGmL5dXF7Pip5Tpk2\", \"qf7BdXXKWTNisXiY3dNGynktezC3\",\n",
        "    \"6AfBZNSFoDYtFZoK3v6IoDzYGX23\", \"DMRsCjYfw3Ui2WY7baPfiliLmYm1\", \"yOwa5JFnOFcPhY68B9JfQ8FXhqN2\",\n",
        "    \"RXrptwB3hBeMx77a0g09or8T7zk2\", \"4N71ZXMaaTcJbEmeUAg1KrZlQNA3\", \"iXcMJZtl9wXfvb5elAKio2Nrn0D2\",\n",
        "    \"OIf29ayJNJUCcBRExfWG27W9iCh1\", \"okJYh5CBOrbVuzpP7dLOiBWxGuj1\", \"i3HmbHac0eQp6D3iC2QMRtNIs7a2\",\n",
        "    \"Et7CFW8jkcX1GRjETBPcGsy42kW2\", \"zFJFxFXuJhTjC7oRQ30u8UJIFLF2\", \"CUFJTwC3SfhFuqCVc3m9r0nRsUp1\",\n",
        "    \"A7v2UgpD3QSG8RSzChNfX5GLha13\", \"vIdx8TsPc4YvjehcrYFnMWskRs73\", \"YZZ3iX1klhOgkdm8iWFqTFgXTe03\",\n",
        "    \"DBoGxzxrMNg35jvii1JkjFFOzQ42\", \"u8GeP9nHKWeyRHkkJG6yjUGMnYY2\", \"V1V9NB0peEOpBgsp8c8eTOYmc6Z2\",\n",
        "    \"a6bTHKiHC8c1Iwtk8fBVGUJVN993\", \"oNu8tY2ELVhmHhUyAwz4jq1GRiQ2\", \"jxjNj9lGrrOSvLg3gxOY5CnxyG03\",\n",
        "    \"7Jnatc1vczRidXKPNsM5LlsVan73\", \"a7VP7VwxbkhH177rO6iBDEgE68v2\", \"bJxW3getIbhJBC1qmBUttjRnlrf1\",\n",
        "    \"zQPKgOM0HsPZLcHcmIxOu6EG8yI3\", \"vD5cfxpwyCZ7mWSkst4eaQ7r9xj2\", \"Xf29cjqMJjO7sQViRDF8x0D20VX2\",\n",
        "    \"afDRYXLOKIVnnWQM5XrTKJzklz42\", \"PNAHYU6Q7ROTcHMQnXou3Tkhj1b2\", \"CyRaDUzppeXweovlWZpUpIpaHth2\",\n",
        "    \"IxtPpUJaN3XSwGxKhk6nwVdRzYC3\", \"CNw5mvk8V2cazrHJdms6PhmRBTu1\",\n",
        "]\n",
        "\n",
        "groups = {\n",
        "    'Control Group': control_group_ids,\n",
        "    'Intervention Group': intervention_group_ids\n",
        "}\n",
        "\n",
        "\n",
        "with pd.ExcelWriter('output.xlsx') as writer:\n",
        "    for group_name, group_ids in groups.items():\n",
        "        all_rows = []\n",
        "        for user_id, user_values in user_data.items():\n",
        "            if user_id in group_ids:\n",
        "                rows = []\n",
        "                known_urls = set()\n",
        "                first_entry_for_user = True  # A flag to keep track of first entry for a user\n",
        "\n",
        "                for date, articles in user_values.items():\n",
        "                    for url_encoded, metrics in articles['articles'].items():\n",
        "                        url = url_encoded.replace('%', '/').replace(' ', '.')\n",
        "                        known_urls.add(url)\n",
        "                        row = [\n",
        "                            url,\n",
        "                            metrics['article_id'],\n",
        "                            metrics['article_read'],\n",
        "                            metrics['avg_reading_time'],\n",
        "                            metrics['click_rate'],\n",
        "                            metrics['current_article_bias'],\n",
        "                            metrics['current_reading_time'],\n",
        "                            convert_time_to_seconds(metrics['current_reading_time']),\n",
        "                            metrics['dominant_bias'],\n",
        "                            metrics['page_title'],\n",
        "                            metrics['scroll_rate'],\n",
        "                            metrics['source'],\n",
        "                            user_id if first_entry_for_user else ''  # Only add UserID for the first row of this user\n",
        "                        ]\n",
        "                        first_entry_for_user = False  # Set the flag to false after the first entry\n",
        "                        rows.append(row)\n",
        "\n",
        "                # Check for additional URLs from user_log\n",
        "                additional_urls = set()\n",
        "                user_log = user_log_data.get(user_id, {})\n",
        "\n",
        "                for date, log in user_log.items():\n",
        "                    for url_encoded in log.get('articles', []):\n",
        "                        url = url_encoded.replace('%', '/').replace(' ', '.')\n",
        "                        if url not in known_urls:\n",
        "                            additional_urls.add(url)\n",
        "\n",
        "                # Append the additional URLs to the rows (if any exist)\n",
        "                if additional_urls:\n",
        "                    # Creating the 'Additional URLs' row\n",
        "                    additional_url_row = ['Additional URLs:'] + list(additional_urls) + [''] * (11 - len(additional_urls))\n",
        "                    if len(rows) == 0:  # If no other rows exist for this user, append the user_id\n",
        "                        additional_url_row.append(user_id)\n",
        "                    else:\n",
        "                        additional_url_row.append('')  # Add an empty space for the UserID column\n",
        "                    rows.append(additional_url_row)\n",
        "\n",
        "                all_rows.extend(rows + [[''] * 13])  # Adding an empty row between different UserIDs\n",
        "\n",
        "        df_group = pd.DataFrame(all_rows, columns=[\n",
        "            \"URL\", \"article_id\", \"article_read\", \"avg_reading_time\", \"click_rate\",\n",
        "            \"current_article_bias\", \"current_reading_time\", \"current_reading_time (seconds)\",\n",
        "            \"dominant_bias\", \"page_title\", \"scroll_rate\", \"source\", \"UserID\"\n",
        "        ])\n",
        "\n",
        "        df_group.to_excel(writer, sheet_name=group_name, index=False)\n",
        "\n",
        "print(\"Data written to output.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaSFSfEtkrW9",
        "outputId": "9c90437e-b9e8-4fe1-dc8d-56609687d376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data written to output.xlsx\n"
          ]
        }
      ]
    }
  ]
}