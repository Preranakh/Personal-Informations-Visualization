{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preranakh/Personal-Informations-Visualization/blob/main/Topic%20Modelling_text_corpus_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following tutorial at https://machinelearninggeek.com/latent-dirichlet-allocation-using-scikit-learn/"
      ],
      "metadata": {
        "id": "ZjNDCpxwBygM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT78pMIh82DG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id='1MI3zrc9Qzte6XU9xsSIC0z0HrYqOUcaZ'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('article_text.csv')"
      ],
      "metadata": {
        "id": "V1imfpf5O8bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"article_text.csv\")"
      ],
      "metadata": {
        "id": "RSR72FO6-GpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_list= df['text'].dropna().tolist() #get documents"
      ],
      "metadata": {
        "id": "uTDPIDgxBhDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate TF-IDF Features\n",
        "\n",
        "# Initialize regex tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Vectorize document using TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase=True,\n",
        "                        stop_words='english',\n",
        "                        ngram_range = (1,1),\n",
        "                        tokenizer = tokenizer.tokenize)\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = tfidf.fit_transform(documents_list)"
      ],
      "metadata": {
        "id": "jmcOx9JABwXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LDA\n",
        "\n",
        "# Define the number of topics or components\n",
        "num_components=5\n",
        "\n",
        "# Create LDA object\n",
        "model=LatentDirichletAllocation(n_components=num_components)\n",
        "\n",
        "# Fit and Transform SVD model on data\n",
        "lda_matrix = model.fit_transform(train_data)\n",
        "\n",
        "# Get Components\n",
        "lda_components=model.components_"
      ],
      "metadata": {
        "id": "19sGAhbhB7uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topics and terms\n",
        "\n",
        "# Print the topics with their terms\n",
        "terms = tfidf.get_feature_names_out()\n",
        "\n",
        "for index, component in enumerate(lda_components):\n",
        "\n",
        "    zipped = zip(terms, component)\n",
        "\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "\n",
        "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
      ],
      "metadata": {
        "id": "bD3wgFndCaDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7feb6528-a4dc-4261-e953-c3f76f94eeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0:  ['dow', 'jones', 'rewards', 'indices', 'garoppolo', 'football', 'aarp']\n",
            "Topic 1:  ['org', 'provider', 'javascript', 'related', 'receive', 'terms', 'aarp']\n",
            "Topic 2:  ['delaware', 'firefly', 'bethany', 'chapter', 'sierra', 'dustyn', 'cd']\n",
            "Topic 3:  ['sale', 'porizkova', 'black', 'sign', 'factset', 'walmart', 'retrograde']\n",
            "Topic 4:  ['s', 'said', 't', 'just', 'republican', 'russia', 'trump']\n"
          ]
        }
      ]
    }
  ]
}